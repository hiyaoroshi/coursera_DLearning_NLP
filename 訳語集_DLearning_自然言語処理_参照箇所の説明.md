### Coursera 字幕翻訳備忘録 訳語集参照箇所の説明

***
- 専門講座名： 自然言語処理 Natural Language Processing Specialization by DeepLearning.AI  

- コース名：
	+ 分類とベクトル空間による自然言語処理 NLP with Classification and Vector Spaces  
    + 確率モデルによる自然言語処理 NLP with Probabilistic Models  
    + シーケンスモデルによる自然言語処理 NLP with Sequence Models  
    + アテンションモデルによる自然言語処理 NLP with Attention Models  
             
- 字幕翻訳作成： nmukai GTC 2020.11
***

(*) 訳語集の参照箇所には下記のビデオ番号をスクリプトで拾って書き込んでいる

1 分類とベクトル空間による自然言語処理 NLP with Classification and Vector Spaces  

1-1 ロジスティック回帰によるセンチメント分析 Sentiment Analysis with Logistic Regression  
		1-1-1 NLP専門講座にようこそ Welcome to the NLP Specialization  
		1-1-2 コースにようこそ Welcome to Course  
		1-1-3 教師付きMLとセンチメント分析 Supervised ML & Sentiment Analysis  
		1-1-4 語彙と属性抽出 Vocabulary & Feature Extraction  
		1-1-5 ネガティブ頻度とポジティブ頻度 Negative and Positive Frequencies  
		1-1-6 頻度による属性抽出 Feature Extraction with Frequencies  
		1-1-7 前処理 Preprocessing  
		1-1-8 全体の処理 Putting it All Together  
		1-1-9 ロジスティック回帰概要 Logistic Regression Overview  
		1-1-10 ロジスティック回帰 トレーニング Logistic Regression: Training  
		1-1-11 ロジスティック回帰 テスト Logistic Regression: Testing  
		1-1-12 ロジスティック回帰 損失関数 Logistic Regression: Cost Function  
		1-1-13 アンドリュー・ングとクリス・マニング Andrew Ng with Chris Manning  

1-2 ナイーズベイズによる感情分析 Sentiment Analysis with Naïve Bayes  
		1-2-1 確率とベイズの定理 Probability and Bayes Rule  
		1-2-2 ベイズの定理 Bayes' Rule  
		1-2-3 ナイーブベイズのイントロNaïve Bayes' Introduction  
		1-2-4 ラプラシアン平滑化 Laplacian Smoothing  
		1-2-5 対数尤度1 Log Likelihood, Part 1  
		1-2-6 対数尤度2 Log Likelihood, Part 2  
		1-2-7 ナイーブベイズのトレーニング Training Naïve Bayes  
		1-2-8 ナイーブベイズのテスト Testing Naïve Bayes  
		1-2-9 ナイーブベイズのアプリケーション Applications of Naïve Bayes  
		1-2-10 ナイーブベイズの前提 Naïve Bayes Assumptions  
		1-2-11 エラー分析 Error Analysis  
		
1-3 ベクトル空間モデル Vector Space Models  
		1-3-1 ベクトル空間モデル Vector Space Models  
		1-3-2 単語by単語と単語by文書 Word by Word and Word by Doc  
		1-3-3 ユークリッド距離 Euclidean Distance  
		1-3-4 コサイン類似度 イントロ Cosine Similarity: Intuition  
		1-3-5 コサイン類似度 Cosine Similarity  
		1-3-6 ベクトル空間での単語操作 Manipulating Words in Vector Spaces  
		1-3-7 視覚化とPCA Visualization and PCA  
		1-3-8 PCAアルゴリズム PCA Algorithm  
	
1-4 機械翻訳と文書検索  
		1-4-1 概要 Overview  
		1-4-2 単語ベクトルの変換 Transforming word vectors  
		1-4-3 K近傍法 K-nearest neighbors  
		1-4-4 ハッシュ表とハッシュ関数 Hash tables and hash functions  
		1-4-5 局所性鋭敏型ハッシング Locality sensitive hashing  
		1-4-6 複数の平面 Multiple Planes  
		1-4-7 近似最近傍 Approximate nearest neighbors  
		1-4-8 文書の検索 Searching documents  
		1-4-9 アンドリュー・ングとキャサリーン・マッキューエン Andrew Ng with Kathleen McKeown  

2 確率モデルによる自然言語処理 NLP with Probabilistic Models  

2-1 自動訂正 Autocorrect  
		2-1-1 コースのイントロ Intro to Course  
		2-1-2 概要 Overview  
		2-1-3 自動訂正 Autocorrect  
		2-1-4 モデルの作成 Building the model  
		2-1-5 モデルの作成2 Building the model II  
		2-1-6 最小編集距離 Minimum edit distance  
		2-1-7 最小編集距離アルゴリズム Minimum edit distance algorithm  
		2-1-8 最小編集距離アルゴリズム2 Minimum edit distance algorithm II  
		2-1-9 最小編集距離アルゴリズム3 Minimum edit distance algorithm III  

2-2 品詞タグ付けと隠れマルコフモデル   
		2-2-1 品詞タグ付け Part of Speech Tagging  
		2-2-2 マルコフチェーン Markov Chains  
		2-2-3 マルコフチェーンと品詞タグ付け Markov Chains and POS Tags  
		2-2-4 隠れマルコフモデル Hidden Markov Models  
		2-2-5 Calculating Probabilities  
		2-2-6 遷移行列への値の設定 Populating the Transition Matrix  
		2-2-7 出力行列への値の設定 Populating the Emission Matrix  
		2-2-8 ビタビアルゴリズム The Viterbi Algorithm  
		2-2-9 ビタビ初期化 Viterbi: Initialization  
		2-2-10 ビダビ前向きパス Viterbi: Forward Path  
		2-2-11 ビタビ後ろ向きパス Viterbi: Backward Path  
	
2-3 自動補完と言語モデル  
		2-3-1 Nグラム概要 N-Grams: Overview  
		2-3-2 Nグラムと確率 N-grams and Probabilities  
		2-3-3 列の確率 Sequence Probabilities  
		2-3-4 文の開始と終了 Starting and Ending Sentences  
		2-3-5 Nグラム言語モデル The N-gram Language Model  
		2-3-6 言語モデルの評価 Language Model Evaluation  
		2-3-7 語彙に無い単語 Out of Vocabulary Words  
		2-3-8 平滑化 Smoothing  
		2-3-9 週のまとめ Week Summary  
	
2-4 ニューラルネットワークによる単語埋め込み   
		2-4-1 概要 Overview  
		2-4-2 基本的な単語表現 Basic Word Representations  
		2-4-3 単語埋込 Word Embeddings  
		2-4-4 単語埋込の作り方 How to Create Word Embeddings  
		2-4-5 単語埋込方法 Word Embedding Methods  
		2-4-6 連続的バグオブワーズモデルContinuous Bag-of-Words Model  
		2-4-7 クリーニングとトークン化 Cleaning and Tokenization  
		2-4-8 Pythonでの単語のスライディング・ウインドウ Sliding Window of Words in Python  
		2-4-9 単語からベクトルへの変換 Transforming Words into Vectors  
		2-4-10 CBOWモデルのアーキテクチャ Architecture of the CBOW Model  
		2-4-11 CBOWモデルのアーキテクチャ 次元 Architecture of the CBOW Model: Dimensions  
		2-4-12 CBOWモデルのアーキテクチャ 次元2 Architecture of the CBOW Model: Dimensions 2  
		2-4-13 CBOWモデルのアーキテクチャ 活性化関数 Architecture of the CBOW Model: Activation Functions  
		2-4-14 CBOWモデルのトレーニング コスト関数 Training a CBOW Model: Cost Function  
		2-4-15 CBOWモデルのトレーニング 前向きパス Training a CBOW Model: Forward Propagation  
		2-4-16 CBOWモデルのトレーニング 後ろ向きパスと勾配降下法 Training a CBOW Model: Backpropagation and Gradient Descent  
		2-4-17 埋込ベクトルの抽出 Extracting Word Embedding Vectors  
		2-4-18 埋込ベクトルの抽出の評価 内的評価 Evaluating Word Embeddings: Intrinsic Evaluation  
		2-4-19 埋込ベクトルの抽出の評価 外的評価 Evaluating Word Embeddings: Extrinsic Evaluation  
		2-4-20 結論 Conclusion  

3 シーケンスモデルによる自然言語処理 NLP with Sequence Models  

3-1 ニューラルネットワークによる感情分析 Neural Networks for Sentiment Analysis  
		3-1-1 コース3のイントロ Course 3 Introduction  
		3-1-2 センティメント分析のためのニューラルネットワーク Neural Networks for Sentiment Analysis  
		3-1-3 ニューラルネットワーク Trax Trax: Neural Networks  
		3-1-4 なぜTraxを推薦するか Why we recommend Trax  
		3-1-5 traxの層 Trax: Layers  
		3-1-6 全結合層とReKU層 Dense and ReLU Layers  
		3-1-7 シリアル層 Serial Layer  
		3-1-8 他の層 Other Layers  
		3-1-9 トレーニング Training  

3-2 RNN による言語モデル Recurrent Neural Networks for Language Modeling  
		3-2-1 伝統的な言語モデル Traditional Language models  
		3-2-2 再帰ニューラルネットワーク Recurrent Neural Networks  
		3-2-3 RNNのアプリケーション Applications of RNNs  
		3-2-4 簡単なRNNの演算 Math in Simple RNNs   
		3-2-5 RNNのコスト関数 Cost Function for RNNs  
		3-2-6 実装についてのメモ Implementation Note  
		3-2-7 ゲート付き再帰ユニット Gated Recurrent Units  
		3-2-8 ディープ双方向RNN Deep and Bi-directional RNNs  

3-3 LSTM と固有表現 LSTMs and Named Entity Recognition  
		3-3-1 RNNと傾斜の消滅 RNNs and Vanishing Gradients  
		3-3-2 LSTMのイントロ Introduction to LSTMs  
		3-3-3 LSTMのアーキテクチャ LSTM Architecture  
		3-3-4 固有表現(NER)のイントロ Introduction to Named Entity Recognition  
		3-3-5 固有表現(NER)のトレーニング データ処理 Training NERs: Data Processing  
		3-3-6 計算精度 Computing Accuracy  

3-4 結合型ネットワーク Siamese Networks  
		3-4-1 結合型ネットワーク Siamese Networks  
		3-4-2 アーキテクチャ Architecture  
		3-4-3 コスト関数 Cost Function  
		3-4-4 トリプレット Triplets  
		3-4-5 コスト計算 Computing The Cost I  
		3-4-6 コスト計算2 Computing The Cost II  
		3-4-7 ワンショット学習 One Shot Learning  
		3-4-8 トレーニングとテスト Training / Testing  

4 アテンションモデルによる自然言語処理 NLP with Attention Models  

4-1 ニューラル機械翻訳 Neural Machine Translation  
		4-1-1 コース4イントロ Course 4 Introduction  
		4-1-2 シーケンス・シーケンスモデル Seq2seq  
		4-1-3 アラインメント Alignment  
		4-1-4 アテンション Attention  
		4-1-5 機械翻訳のための設定 Setup for Machine Translation  
		4-1-6 アテンション付きNMTのトレーニング Training an NMT with Attention  
		4-1-7 機械翻訳の評価 Evaluation for Machine Translation  
		4-1-8 サンプリングとデコード Sampling and Decoding  
		4-1-9 アンドリュー・ングとオレン・エツィオーニ Andrew Ng with Oren Etzioni  
	
4-2 テキストの要約 Text Summarization  
		4-2-1 トランスフォーマとRNN Transformers vs RNNs  
		4-2-2 トランスフォーマのアプリケーション Transformer Applications  
		4-2-3 ドット積アテンション Dot-Product Attention  
		4-2-4 因果的アテンション Causal Attention  
		4-2-5 マルチヘッドアテンション Multi-head Attention  
		4-2-6 トランスフォーマデコーダ Transformer Decoder  
		4-2-7 トランスフォーマによる要約作成 Transformer Summarizer  

4-3 質問への回答 Question Answering  
		4-3-1 第3週の要約 Week 3 Overview  
		4-3-2 NLPでの転移学習 Transfer Learning in NLP  
		4-3-3 ELMo GPTBERT T5 ELMo, GPT, BERT, T5  
		4-3-4 BERT Bidirectional Encoder Representations from Transformers (BERT)  
		4-3-5 BERTの目的 BERT Objective  
		4-3-6 BERTの微調整 Fine tuning BERT  
		4-3-7 トランスフォーマT5 Transformer: T5  
		4-3-8 マルチタスクトレーニング戦略 Multi-Task Training Strategy  
		4-3-9 BLUEベンチマーク GLUE Benchmark  
		4-3-10 質問への回答 Question Answering  
	
4-4 チャットボット Chatbot  
		4-4-1 長いシーケンスのタスク Tasks with Long Sequences  
		4-4-2 トランスフォーマの複雑性 Transformer Complexity  
		4-4-3 LSHアテンション LSH Attention  
		4-4-4 可逆層の動機 メモリー Motivation for Reversible Layers: Memory!  
		4-4-5 可逆残差層 Reversible Residual Layers  
		4-4-6 リフォーマ Reformer  
		4-4-7 アンドリュー・ングとクオック・ラー Andrew Ng with Quoc Le  
